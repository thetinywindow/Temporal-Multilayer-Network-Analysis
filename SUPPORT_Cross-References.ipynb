{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopandas\n",
      "  Using cached biopandas-0.2.5-py2.py3-none-any.whl (263 kB)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from biopandas) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from biopandas) (45.2.0.post20200210)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from biopandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas>=0.24.2->biopandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from pandas>=0.24.2->biopandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->biopandas) (1.14.0)\n",
      "Installing collected packages: biopandas\n",
      "Successfully installed biopandas-0.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install biopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Using cached biopython-1.76-cp36-cp36m-manylinux1_x86_64.whl (2.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from biopython) (1.18.1)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.76\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install biopython --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "\n",
    "import sherlockml.datasets as sfs\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#sfs.get('/input/spotfunc.py', 'spotfunc.py')\n",
    "#sfs.get('/input/playlists_ids_and_titles.csv', 'playlists_ids_and_titles.csv')\n",
    "#sfs.get('/input/new artists2015onwards.csv', 'newartists2015onwards.csv')\n",
    "#sfs.get('/input/cleaned_data.csv', 'cleaned_data.csv')\n",
    "\n",
    "# Add more stuff here as necessary \n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Import all required libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find Referenc List for PMID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Make it work for a list of PMIDs </b> \n",
    "\n",
    "Should work for a lists of 5000 PMIDs. Should give out a dataframe with a row with the original PMIDs and the references in the columns.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"anina.carstens@gmx.de\"\n",
    "def get_links_id(pmid):\n",
    "    link_list = []\n",
    "    links = Entrez.elink(dbfrom=\"pubmed\", id=pmid, linkname=\"pubmed_pubmed_refs\")\n",
    "    record = Entrez.read(links)\n",
    "\n",
    "    records = record[0][u'LinkSetDb'][0][u'Link']\n",
    "\n",
    "    for link in records:\n",
    "        link_list.append(link[u'Id'])\n",
    "\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24803944', '24731885', '24701238', '24175774', '24125973', '23886196', '23874588', '23150723', '23142798', '23036669', '23033982', '22211126', '21875474', '21875285', '21430075', '21133622', '20354574', '18383896', '17089041', '15154663', '15015612', '14626183', '11489752']\n"
     ]
    }
   ],
   "source": [
    "x = get_links_id(26816981)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find Author Names of References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Make it work for approximately 500,000 PMIDs.  </b> \n",
    "\n",
    "Should work for 500,000 PMIDs. Give out DataFrame with multiindex. Right now only working for a single PDMI at a time\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PubMedScraper(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 proxy = None):\n",
    "        \n",
    "        \n",
    "        self.proxies = {'https' : proxy}\n",
    "        self.base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
    "        self.db = 'pubmed'\n",
    "        self.retmode = 'xml'\n",
    "    \n",
    "    def __query_builder(self,\n",
    "                      input_keywords):\n",
    "        return input_keywords.replace(' ','+')\n",
    "    \n",
    "    def search(self,\n",
    "               query,\n",
    "               max_results=15000):\n",
    "        \n",
    "        if ' ' in query:\n",
    "            query = self.__query_builder(query)\n",
    "        \n",
    "        search_url = self.base + 'esearch.fcgi?db=' + self.db + '&retmode=' + \\\n",
    "        self.retmode + '&retmax=' + str(max_results) + '&term=' + query + \\\n",
    "        '&sort=relevance'+'&usehistory=y'\n",
    "\n",
    "        search_response = requests.get(search_url,proxies = self.proxies)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        soup = bs(search_response.text, 'xml')\n",
    "        \n",
    "        webenv = soup.find('WebEnv').text\n",
    "        qkey = soup.find('QueryKey').text\n",
    "        article_count = int(soup.find('RetMax').text)\n",
    "        \n",
    "        if article_count > 10000:\n",
    "            \n",
    "            self.results = list()\n",
    "            \n",
    "            for retmin in range(0,article_count,10000):\n",
    "\n",
    "                if (retmin + 10000) > article_count:\n",
    "                    retmax = article_count\n",
    "                    print('Making an API request for articles numbered ' + str(retmin)+ ' to ' + str(retmax))\n",
    "                else:\n",
    "                    retmax = retmin + 9999\n",
    "                    print('Making an API request for articles numbered ' + str(retmin)+ ' to ' + str(retmax))\n",
    "                    \n",
    "                fetch_url = self.base + 'efetch.fcgi?db=' + self.db + \\\n",
    "                '&query_key=' + qkey + '&WebEnv=' + webenv +'&retmin=' + \\\n",
    "                str(retmin) +'&retmax='+\\\n",
    "                str(retmax)+'&rettype=xml&retmode=xml'\n",
    "                \n",
    "                fetch_response = requests.get(fetch_url, proxies = self.proxies)\n",
    "                \n",
    "                time.sleep(5)\n",
    "                \n",
    "                self.results.append(bs(fetch_response.text, 'xml'))\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            fetch_url = self.base + 'efetch.fcgi?db=' + self.db + \\\n",
    "            '&query_key=' + qkey + '&WebEnv=' + webenv + '&retmax='+\\\n",
    "            str(article_count)+'&rettype=xml&retmode=xml'\n",
    "            \n",
    "            fetch_response = requests.get(fetch_url, proxies = self.proxies)\n",
    "            \n",
    "            fetch_soup = bs(fetch_response.text, 'xml')\n",
    "            \n",
    "            self.results = [fetch_soup]\n",
    "\n",
    "    \n",
    "    def __content_extract(self,\n",
    "                       article):\n",
    "        \n",
    "        contents = list()\n",
    "        \n",
    "        contents.append(article.find('ArticleTitle').text) #Title\n",
    "        contents.append(article.find('PMID').text)\n",
    "        try:\n",
    "            contents.append(article.find('ArticleDate').find('Year').text + '-' +\\\n",
    "            article.find('ArticleDate').find('Month').text + '-' +\\\n",
    "            article.find('ArticleDate').find('Day').text)\n",
    "        except:\n",
    "            try:\n",
    "                contents.append(article.find('History').find('Year').text + '-' +\\\n",
    "                article.find('History').find('Month').text + '-' +\\\n",
    "                article.find('History').find('Day').text)\n",
    "            except:\n",
    "                contents.append('')\n",
    "        \n",
    "        author_list = article.find_all('Author')\n",
    "        if len(author_list) == 0:\n",
    "            contents.append('')\n",
    "            contents.append('')\n",
    "            contents.append('')\n",
    "            contents.append('')\n",
    "            \n",
    "        elif len(author_list) == 1:\n",
    "            try:\n",
    "                fname = author_list[0].find('ForeName').text\n",
    "            except:\n",
    "                fname = ''\n",
    "            try:\n",
    "                lname = author_list[0].find('LastName').text\n",
    "            except:\n",
    "                lname = ''\n",
    "            contents.append(fname + ' ' + lname)\n",
    "            \n",
    "            try:\n",
    "                contents.append(author_list[0].find('Affiliation').text)\n",
    "            except:\n",
    "                contents.append('')\n",
    "            contents.append('')\n",
    "            contents.append('')\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                fname = author_list[0].find('ForeName').text\n",
    "            except:\n",
    "                fname = ''\n",
    "            try:\n",
    "                lname = author_list[0].find('LastName').text\n",
    "            except:\n",
    "                lname = ''\n",
    "            contents.append(fname + ' ' + lname)\n",
    "            \n",
    "            try:\n",
    "                contents.append(author_list[0].find('Affiliation').text)\n",
    "            except:\n",
    "                contents.append('')\n",
    "            authors = ''\n",
    "            insts = ''\n",
    "            for i,author in enumerate(author_list):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    authors += author.find('ForeName').text + ' ' + \\\n",
    "                    author.find('LastName').text + ', '\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    insts += author.find('Affiliation').text + '_ '\n",
    "                except:\n",
    "                    pass\n",
    "            contents.append(authors.strip(', '))\n",
    "            contents.append(insts.strip(', '))\n",
    "        \n",
    "        keywords = article.find_all('Keyword')\n",
    "        if len(keywords) >= 1:\n",
    "            contents.append(', '.join([kw.text for kw in keywords]))\n",
    "        else:\n",
    "            contents.append('')\n",
    "        \n",
    "        contents.append(article.find('Journal').find('Title').text)\n",
    "        \n",
    "        contents.append(' '.join([abst.text for abst in article.find_all('AbstractText')]).replace('\\n',''))\n",
    "        \n",
    "        contents = ['\"' + content.replace('\"','').replace('\\n','').strip() + '\"' for content in contents]\n",
    "        return contents\n",
    "            \n",
    "       \n",
    "    \n",
    "    def saveas(self,\n",
    "               file = '',\n",
    "               filename=''):\n",
    "        \n",
    "        ### Still broken, needs fixing\n",
    "        if file == 'xml':\n",
    "            for result in self.results:\n",
    "                with open(filename,'w') as f:\n",
    "                    f.write(\"\\n\".join(str(result,encoding='utf-8') ))\n",
    "        ###\n",
    "        if file == 'csv':\n",
    "            article_line = list()\n",
    "            \n",
    "            for result in self.results:\n",
    "                for article in result.find_all('PubmedArticle'):\n",
    "                    article_line.append(self.__content_extract(article))\n",
    "                    \n",
    "            with open(filename, 'w',encoding='utf-8') as f:\n",
    "                f.write('\"Article Title\",\"PMID\", \"Published Date\",\"Main Author\",\"Main Author Affiliation\",\"Secondary Author(s)\",\"Secondary Author(s) Affiliation\",\"Keywords\",\"Journal Name\",\"Abstract\"\\n')\n",
    "                for row in article_line:\n",
    "                    f.write(','.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = PubMedScraper()\n",
    "scraper.search(query = '26816981', max_results = 1)\n",
    "\n",
    "scraper.saveas(file = 'csv', filename = 'Test2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 9.54 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Title</th>\n",
       "      <th>PMID</th>\n",
       "      <th>\"Published Date\"</th>\n",
       "      <th>Main Author</th>\n",
       "      <th>Main Author Affiliation</th>\n",
       "      <th>Secondary Author(s)</th>\n",
       "      <th>Secondary Author(s) Affiliation</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Journal Name</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phytomedicine in the Treatment of Cancer: A He...</td>\n",
       "      <td>26816981</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>Tanushree Chaudhary</td>\n",
       "      <td>HTA Fellow, Centre for Health Technology Asses...</td>\n",
       "      <td>Akriti Chahar, Jitendar Kumar Sharma, Kirandee...</td>\n",
       "      <td>Research Associate, Health Technology Assessme...</td>\n",
       "      <td>Health economics, Herbal medicine</td>\n",
       "      <td>Journal of clinical and diagnostic research : ...</td>\n",
       "      <td>Cancer is reported to cause about 0.4 million ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Article Title      PMID  \\\n",
       "0  Phytomedicine in the Treatment of Cancer: A He...  26816981   \n",
       "\n",
       "   \"Published Date\"          Main Author  \\\n",
       "0        2015-12-01  Tanushree Chaudhary   \n",
       "\n",
       "                             Main Author Affiliation  \\\n",
       "0  HTA Fellow, Centre for Health Technology Asses...   \n",
       "\n",
       "                                 Secondary Author(s)  \\\n",
       "0  Akriti Chahar, Jitendar Kumar Sharma, Kirandee...   \n",
       "\n",
       "                     Secondary Author(s) Affiliation  \\\n",
       "0  Research Associate, Health Technology Assessme...   \n",
       "\n",
       "                            Keywords  \\\n",
       "0  Health economics, Herbal medicine   \n",
       "\n",
       "                                        Journal Name  \\\n",
       "0  Journal of clinical and diagnostic research : ...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  Cancer is reported to cause about 0.4 million ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Read in sampled data\n",
    "data = pd.read_csv('Test2')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Schleife einfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "########################################################################################\n",
    "#insert timeframe of interest\n",
    "start_date = date(2019,1,1)\n",
    "end_date = date(2020,1,1)\n",
    "########################################################################################\n",
    "\n",
    "appended_data = []\n",
    "appended_colnames = []\n",
    "\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    #full date\n",
    "    run_time = str(single_date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    #date split into year, month, day\n",
    "    year_run = int(single_date.strftime('%Y'))\n",
    "    month_run = int(single_date.strftime('%m'))\n",
    "    day_run = int(single_date.strftime('%d'))\n",
    "    \n",
    "    #apply function to calculate 52 week high\n",
    "    df_final = final_func(year_run,month_run,day_run, data)\n",
    "    \n",
    "    #append dataframe w/ 52 week highs for each day\n",
    "    appended_data.append(df_final)\n",
    "    \n",
    "    #append list of dates (aka column names)\n",
    "    appended_colnames.append(run_time)\n",
    "\n",
    "#merge dataframes    \n",
    "appended_data = pd.concat(appended_data,axis=1, sort=False)\n",
    "#rename columns with dates\n",
    "appended_data.columns = appended_colnames\n",
    "\n",
    "#showcase final resulrs\n",
    "appended_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
